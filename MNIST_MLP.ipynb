{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" MNIST \n",
    "    MLP \n",
    "    MODEL\"\"\"\n",
    "\n",
    "# import\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed ‘logdir/events.out.tfevents.1504842330.student13’\r\n",
      "removed directory: ‘logdir’\r\n"
     ]
    }
   ],
   "source": [
    "# Set Placeholders\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28], name='X')\n",
    "Y = tf.placeholder(tf.int64, shape=[None], name='Y')\n",
    "Y_onehot = tf.one_hot(Y, 10, axis=1, name='Y_onehot')\n",
    "\n",
    "training1 = tf.placeholder(tf.bool, name = 'training1')\n",
    "\n",
    "# Set Variables\n",
    "# Set hidden Layer1\n",
    "with tf.name_scope(\"Layer1\"):\n",
    "    W1 = tf.Variable(tf.truncated_normal([784, 300], stddev=0.02), name='W1')\n",
    "    B1 = tf.Variable(tf.zeros([300]), name = 'B1')\n",
    "    H1 = tf.matmul(X, W1) + B1\n",
    "    H1 = tf.nn.relu(H1, name='relu1')\n",
    "    H1 = tf.layers.dropout(H1, rate=0.5, training = training1, name='train1')\n",
    "\n",
    "# Set hidden Layer2\n",
    "\n",
    "with tf.name_scope(\"Layer2\"):\n",
    "    W2 = tf.Variable(tf.truncated_normal([300,200], stddev=0.02), name='W2')\n",
    "    B2 = tf.Variable(tf.zeros([200]), name='B2')\n",
    "    H2 = tf.matmul(H1, W2) + B2\n",
    "    H2 = tf.nn.relu(H2, name='relu2')\n",
    "    H2 = tf.layers.dropout(H2, rate=0.5, training = training1, name='train2' )\n",
    "\n",
    "# Set Last Layer\n",
    "\n",
    "with tf.name_scope(\"Layer3\"):\n",
    "    W3 = tf.Variable(tf.truncated_normal([200,10], stddev=0.02), name='W3')\n",
    "    B3 = tf.Variable(tf.zeros([10]), name='B3')\n",
    "\n",
    "    Y_pred = tf.matmul(H2, W3) + B3\n",
    "    Y_pred = tf.layers.dropout(Y_pred, rate=0.5, training = training1, name='lastLayer' )\n",
    "\n",
    "    # Set Loss Function(Softmax)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Y_pred, labels=Y_onehot))\n",
    "\n",
    "# Set SoftMax & Accuracy Function\n",
    "\n",
    "with tf.name_scope(\"Softmax\"):\n",
    "    softmax = tf.nn.softmax(logits=Y_pred)\n",
    "    correct = tf.equal(tf.argmax(softmax,1), Y)\n",
    "    acc = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# Set Trainer & Optimizer\n",
    "\n",
    "with tf.name_scope('Optimizer'):\n",
    "    trainer = tf.train.GradientDescentOptimizer(learning_rate=0.05, name='trainer')\n",
    "    optimizer = trainer.minimize(loss, name='optimizer')\n",
    "    \n",
    "\n",
    "tf.summary.scalar('loss', loss)\n",
    "tf.summary.scalar('accuracy',acc)\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "!rm -fvr logdir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot_Loss = 2.19926, Train_Accuracy = 0.21980, Test Accuracy = 61.77%\n",
      "tot_Loss = 1.67986, Train_Accuracy = 0.40578, Test Accuracy = 81.72%\n",
      "tot_Loss = 1.43103, Train_Accuracy = 0.47525, Test Accuracy = 86.05%\n",
      "tot_Loss = 1.32421, Train_Accuracy = 0.50607, Test Accuracy = 88.47%\n",
      "tot_Loss = 1.26276, Train_Accuracy = 0.51938, Test Accuracy = 89.69%\n",
      "tot_Loss = 1.22163, Train_Accuracy = 0.52972, Test Accuracy = 90.70%\n",
      "tot_Loss = 1.18694, Train_Accuracy = 0.53602, Test Accuracy = 91.11%\n",
      "tot_Loss = 1.16005, Train_Accuracy = 0.54240, Test Accuracy = 91.57%\n",
      "tot_Loss = 1.15073, Train_Accuracy = 0.54388, Test Accuracy = 92.37%\n",
      "tot_Loss = 1.11946, Train_Accuracy = 0.55058, Test Accuracy = 92.66%\n",
      "tot_Loss = 1.11206, Train_Accuracy = 0.55175, Test Accuracy = 93.02%\n",
      "tot_Loss = 1.08910, Train_Accuracy = 0.55863, Test Accuracy = 93.30%\n",
      "tot_Loss = 1.07995, Train_Accuracy = 0.56000, Test Accuracy = 93.69%\n",
      "tot_Loss = 1.07887, Train_Accuracy = 0.55805, Test Accuracy = 93.71%\n",
      "tot_Loss = 1.06537, Train_Accuracy = 0.56203, Test Accuracy = 94.10%\n",
      "tot_Loss = 1.05766, Train_Accuracy = 0.56337, Test Accuracy = 94.30%\n",
      "tot_Loss = 1.04649, Train_Accuracy = 0.56530, Test Accuracy = 94.64%\n",
      "tot_Loss = 1.03888, Train_Accuracy = 0.56588, Test Accuracy = 94.70%\n",
      "tot_Loss = 1.03537, Train_Accuracy = 0.56803, Test Accuracy = 94.89%\n",
      "tot_Loss = 1.03050, Train_Accuracy = 0.56837, Test Accuracy = 95.00%\n",
      "tot_Loss = 1.02992, Train_Accuracy = 0.56828, Test Accuracy = 95.22%\n",
      "tot_Loss = 1.02048, Train_Accuracy = 0.57097, Test Accuracy = 95.37%\n",
      "tot_Loss = 1.01099, Train_Accuracy = 0.57155, Test Accuracy = 95.54%\n",
      "tot_Loss = 1.00972, Train_Accuracy = 0.57325, Test Accuracy = 95.56%\n",
      "tot_Loss = 1.00848, Train_Accuracy = 0.57083, Test Accuracy = 95.70%\n",
      "tot_Loss = 1.00209, Train_Accuracy = 0.57377, Test Accuracy = 95.66%\n",
      "tot_Loss = 0.99561, Train_Accuracy = 0.57500, Test Accuracy = 95.77%\n",
      "tot_Loss = 0.99870, Train_Accuracy = 0.57260, Test Accuracy = 96.02%\n",
      "tot_Loss = 0.99083, Train_Accuracy = 0.57553, Test Accuracy = 96.05%\n",
      "tot_Loss = 0.98657, Train_Accuracy = 0.57592, Test Accuracy = 96.19%\n",
      "tot_Loss = 0.99333, Train_Accuracy = 0.57360, Test Accuracy = 96.41%\n",
      "tot_Loss = 0.98092, Train_Accuracy = 0.57713, Test Accuracy = 96.44%\n",
      "tot_Loss = 0.97844, Train_Accuracy = 0.57748, Test Accuracy = 96.39%\n",
      "tot_Loss = 0.97416, Train_Accuracy = 0.57965, Test Accuracy = 96.35%\n",
      "tot_Loss = 0.97390, Train_Accuracy = 0.57840, Test Accuracy = 96.52%\n",
      "tot_Loss = 0.97126, Train_Accuracy = 0.57870, Test Accuracy = 96.49%\n",
      "tot_Loss = 0.97646, Train_Accuracy = 0.57562, Test Accuracy = 96.62%\n",
      "tot_Loss = 0.96230, Train_Accuracy = 0.58165, Test Accuracy = 96.67%\n",
      "tot_Loss = 0.97192, Train_Accuracy = 0.57723, Test Accuracy = 96.69%\n",
      "tot_Loss = 0.96613, Train_Accuracy = 0.57747, Test Accuracy = 96.79%\n",
      "tot_Loss = 0.96432, Train_Accuracy = 0.57857, Test Accuracy = 96.80%\n",
      "tot_Loss = 0.95766, Train_Accuracy = 0.58123, Test Accuracy = 96.87%\n",
      "tot_Loss = 0.95854, Train_Accuracy = 0.58120, Test Accuracy = 96.90%\n",
      "tot_Loss = 0.96223, Train_Accuracy = 0.57908, Test Accuracy = 96.89%\n",
      "tot_Loss = 0.95739, Train_Accuracy = 0.58158, Test Accuracy = 97.05%\n",
      "tot_Loss = 0.95374, Train_Accuracy = 0.58145, Test Accuracy = 97.01%\n",
      "tot_Loss = 0.95659, Train_Accuracy = 0.57947, Test Accuracy = 97.15%\n",
      "tot_Loss = 0.95312, Train_Accuracy = 0.58087, Test Accuracy = 97.16%\n",
      "tot_Loss = 0.95267, Train_Accuracy = 0.58100, Test Accuracy = 97.10%\n",
      "tot_Loss = 0.94958, Train_Accuracy = 0.58097, Test Accuracy = 97.18%\n"
     ]
    }
   ],
   "source": [
    "# Data Loading & Normalization\n",
    "# image file has pixel data from 0016 -> reshape(60000,28,28), float\n",
    "# label file has label data from 0008 -> reshape(60000), int\n",
    "\n",
    "fd = open('./mnist/train-images.idx3-ubyte')\n",
    "images = np.fromfile(file=fd, dtype=np.uint8)\n",
    "images = images[16:].reshape([60000,28*28]).astype(np.float)\n",
    "images = images/127. -1.\n",
    "\n",
    "#print(images[1,])\n",
    "fd = open('./mnist/train-labels.idx1-ubyte')\n",
    "labels = np.fromfile(file=fd, dtype=np.uint8)\n",
    "labels = labels[8:].reshape([60000]).astype(np.int)\n",
    "\n",
    "# Test Data Loading\n",
    "fd = open('./mnist/t10k-images.idx3-ubyte')\n",
    "t_images = np.fromfile(file=fd, dtype=np.uint8)\n",
    "t_images = t_images[16:].reshape([10000,28*28]).astype(np.float)\n",
    "t_images = t_images/127. -1.\n",
    "\n",
    "fd = open('./mnist/t10k-labels.idx1-ubyte')\n",
    "t_labels = np.fromfile(file=fd, dtype=np.uint8)\n",
    "t_labels = t_labels[8:].reshape([10000]).astype(np.int)\n",
    "\n",
    "# Session Create\n",
    "\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "    \n",
    "# variable initialize\n",
    "sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "# Set batch size\n",
    "batch_size = 500\n",
    "batch_count = 60000 // batch_size\n",
    "test_count = 10000 // batch_size\n",
    "    \n",
    "writer = tf.summary.FileWriter('logdir', graph = tf.get_default_graph())\n",
    "    \n",
    "step = 1\n",
    "# Epoch Loop\n",
    "for epoch in range(50) :\n",
    "        \n",
    "    # Set loss, acc, t_acc\n",
    "    total_loss = 0.\n",
    "    total_acc = 0.\n",
    "    total_t_acc = 0.\n",
    "    # Mini-Batch Loop\n",
    "    for i in range(batch_count):\n",
    "           \n",
    "        # Set batch_index\n",
    "        batch_index = i * batch_size\n",
    "        \n",
    "        img = np.reshape(images[batch_index : batch_index + batch_size], [batch_size, 784])\n",
    "        label = labels[batch_index : batch_index + batch_size]\n",
    "            \n",
    "        # Session run\n",
    "        _, loss_v, acc_v, summary_v = sess.run([optimizer, loss, acc, summary_op], feed_dict={X:img, Y:label, training1:True})\n",
    "            \n",
    "        total_loss += loss_v\n",
    "        total_acc += acc_v\n",
    "            \n",
    "        writer.add_summary(global_step=step, summary=summary_v)\n",
    "        step += 1\n",
    "        \n",
    "    total_loss = total_loss / batch_count\n",
    "    total_acc= total_acc / batch_count\n",
    "        \n",
    "    # Set Test Loop\n",
    "    for j in range(test_count) :\n",
    "            \n",
    "        # Set batch_index\n",
    "        test_index = j * batch_size\n",
    "            \n",
    "        t_img = np.reshape(t_images[test_index : test_index + batch_size], [batch_size, 784])\n",
    "        t_label = t_labels[test_index : test_index + batch_size]\n",
    "            \n",
    "        # Session run\n",
    "        t_acc_v, summary_t = sess.run([acc, summary_op], feed_dict={X:t_img, Y:t_label, training1:False})\n",
    "            \n",
    "        total_t_acc += t_acc_v\n",
    "            \n",
    "        writer.add_summary(global_step=step, summary=summary_t)\n",
    "        step += 1\n",
    "        \n",
    "    total_t_acc = total_t_acc / test_count\n",
    "        \n",
    "    print('tot_Loss = %.5f, Train_Accuracy = %.5f, Test Accuracy = %.2f%%' % (total_loss, total_acc, total_t_acc*100) )\n",
    "        \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mW0908 12:50:43.432061 Reloader plugin_event_accumulator.py:303] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mTensorBoard 0.1.5 at http://student13:6006 (Press CTRL+C to quit) ^C\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
